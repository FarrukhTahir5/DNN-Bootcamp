{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT4DAD4Qx3RC"
      },
      "source": [
        "# **BOOTCAMP @ GIKI (Content designed by Usama Arshad) WEEK 3**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRzVwr88x6HD"
      },
      "source": [
        "Week 3: Day 14 - Foundations of CNNs (Object Detection - Yolo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrIuf2PBt7IG"
      },
      "source": [
        "## YOLO: You Only Look Once\n",
        "\n",
        "### What is YOLO?\n",
        "\n",
        "YOLO (You Only Look Once) is a popular real-time object detection algorithm. It can detect multiple objects in images or videos and draw bounding boxes around them. YOLO is known for its speed and accuracy, making it suitable for real-time applications.\n",
        "\n",
        "### How Does YOLO Work?\n",
        "\n",
        "YOLO treats object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Here's a simplified explanation:\n",
        "\n",
        "1. **Input Image**: The input image is divided into a grid of S x S cells.\n",
        "2. **Bounding Boxes**: Each grid cell predicts B bounding boxes and their confidence scores.\n",
        "3. **Class Probabilities**: Each grid cell also predicts class probabilities for the object.\n",
        "4. **Combining Predictions**: YOLO combines these predictions to produce final bounding boxes with associated class labels.\n",
        "\n",
        "### Advantages of YOLO\n",
        "\n",
        "- **Speed**: YOLO is incredibly fast because it makes predictions with a single network pass.\n",
        "- **Accuracy**: It achieves high accuracy by considering contextual information in predictions.\n",
        "- **Simplicity**: YOLO's design is simple, making it easy to understand and implement.\n",
        "\n",
        "### Different Versions of YOLO\n",
        "\n",
        "Over the years, YOLO has evolved through several versions, each improving upon the last.\n",
        "\n",
        "#### YOLOv1\n",
        "\n",
        "- **Introduction**: The first version of YOLO, introduced in 2016.\n",
        "- **Architecture**: Uses a single convolutional neural network (CNN) to predict bounding boxes and class probabilities directly.\n",
        "- **Speed**: Achieves real-time processing speeds.\n",
        "\n",
        "#### YOLOv2 (YOLO9000)\n",
        "\n",
        "- **Introduction**: Released in 2017, also known as YOLO9000.\n",
        "- **Improvements**:\n",
        "  - **Batch Normalization**: Helps in stabilizing the training process and improving accuracy.\n",
        "  - **Anchor Boxes**: Introduces anchor boxes for better bounding box predictions.\n",
        "  - **Multi-Scale Training**: Trains the model at different scales to improve performance.\n",
        "- **Speed and Accuracy**: Better balance between speed and accuracy compared to YOLOv1.\n",
        "\n",
        "#### YOLOv3\n",
        "\n",
        "- **Introduction**: Released in 2018.\n",
        "- **Improvements**:\n",
        "  - **Darknet-53 Backbone**: Uses a deeper network (53 convolutional layers) for feature extraction.\n",
        "  - **Multi-Scale Predictions**: Makes predictions at three different scales to detect objects of various sizes.\n",
        "  - **Improved Bounding Box Predictions**: Enhances the accuracy of bounding box predictions.\n",
        "- **Performance**: Improved performance and accuracy over YOLOv2.\n",
        "\n",
        "#### YOLOv4\n",
        "\n",
        "- **Introduction**: Released in 2020.\n",
        "- **Improvements**:\n",
        "  - **CSPDarknet53 Backbone**: Uses Cross-Stage Partial connections for better feature extraction.\n",
        "  - **Bag of Freebies (BoF)**: Includes various training techniques that improve accuracy without increasing inference time.\n",
        "  - **Bag of Specials (BoS)**: Includes additional layers and modules that enhance performance.\n",
        "- **State-of-the-Art**: Achieves state-of-the-art performance in real-time object detection.\n",
        "\n",
        "#### YOLOv5\n",
        "\n",
        "- **Introduction**: Developed by Ultralytics and released in 2020.\n",
        "- **Improvements**:\n",
        "  - **PyTorch Implementation**: Implements YOLO in the PyTorch framework, making it more accessible.\n",
        "  - **Ease of Use**: Provides a user-friendly interface and pre-trained models for quick deployment.\n",
        "  - **Enhanced Features**: Includes various improvements in training techniques and model architecture.\n",
        "- **Popularity**: Widely adopted due to its ease of use and integration with PyTorch.\n",
        "\n",
        "#### YOLOv6 and Beyond\n",
        "\n",
        "- **Continued Evolution**: YOLO continues to evolve with new versions being developed, incorporating advancements in neural network architectures and training techniques.\n",
        "- **Focus Areas**: Emphasis on improving accuracy, speed, and robustness in real-world applications.\n",
        "\n",
        "### Applications of YOLO\n",
        "\n",
        "YOLO is used in a variety of applications due to its speed and accuracy:\n",
        "\n",
        "- **Autonomous Vehicles**: Detecting pedestrians, vehicles, and other objects in real-time.\n",
        "- **Surveillance**: Monitoring security cameras for suspicious activities.\n",
        "- **Robotics**: Enabling robots to perceive and interact with their environment.\n",
        "- **Healthcare**: Assisting in medical imaging analysis.\n",
        "- **Augmented Reality**: Enhancing real-world experiences with virtual objects.\n",
        "\n",
        "### Summary\n",
        "\n",
        "YOLO has revolutionized the field of object detection with its real-time capabilities and high accuracy. Each version of YOLO has brought significant improvements, making it a versatile and powerful tool for various applications. Understanding the different versions and their enhancements helps in selecting the right model for specific use cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gsVu-OoyE4X"
      },
      "source": [
        "![Yolo Main Structure](https://projectgurukul.org/wp-content/uploads/2022/01/yolo-cnn.webp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNyENHtGuTqb",
        "outputId": "f01a7984-0705-4d09-8fcf-cf9290f522ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /Users/student/anaconda3/lib/python3.9/site-packages (4.7.0.68)\n",
            "Requirement already satisfied: pillow in /Users/student/anaconda3/lib/python3.9/site-packages (10.0.1)\n",
            "Requirement already satisfied: numpy in /Users/student/.local/lib/python3.9/site-packages (1.23.5)\n",
            "Collecting tk\n",
            "  Using cached tk-0.1.0-py3-none-any.whl.metadata (693 bytes)\n",
            "Using cached tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: tk\n",
            "Successfully installed tk-0.1.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python pillow numpy tk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTxxY0C1xXJB"
      },
      "source": [
        "**Yolo v3 CFG file:**\n",
        "https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
        "\n",
        "**Yolo v3 Names File**\n",
        "https://github.com/pjreddie/darknet/blob/master/data/coco.names\n",
        "\n",
        "**Yolo v3 Weights**\n",
        "https://github.com/patrick013/Object-Detection---Yolov3/blob/master/model/yolov3.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoxKWTTU2FjD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Note: Code must be run in offline ide.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "oYNJMGe9t56O",
        "outputId": "e56cc75a-909d-411e-a7ac-94dfdff4e813"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Importing required modules for GUI, image processing, and threading\n",
        "import tkinter as tk  # GUI library\n",
        "from tkinter import filedialog, messagebox  # File dialog and message box\n",
        "from PIL import Image, ImageTk  # Image handling\n",
        "import cv2  # OpenCV library for image processing\n",
        "import numpy as np  # NumPy library for numerical operations\n",
        "import threading  # Threading library for parallel processing\n",
        "\n",
        "class YOLOFaceDetectionApp:\n",
        "    def __init__(self, root):\n",
        "        \"\"\"\n",
        "        Initialize the YOLO Face Detection App\n",
        "        \"\"\"\n",
        "        self.root = root  # Tkinter root window\n",
        "        self.root.title(\"YOLO Object Detection and Filters\")  # Set window title\n",
        "\n",
        "        # File paths for YOLO model files\n",
        "        self.weights_path = \"\"\n",
        "        self.cfg_path = \"\"\n",
        "        self.names_path = \"\"\n",
        "\n",
        "        # GUI elements\n",
        "        self.panel = tk.Label(root)  # Label to display images/videos\n",
        "        self.panel.pack(padx=10, pady=10)  # Add padding around the label\n",
        "\n",
        "        btn_frame = tk.Frame(root)  # Frame to hold buttons\n",
        "        btn_frame.pack(fill=tk.X, pady=10)  # Add padding and fill horizontally\n",
        "\n",
        "        # Buttons for various functionalities\n",
        "        btn_select_weights = tk.Button(btn_frame, text=\"Select Weights\", command=self.select_weights)\n",
        "        btn_select_weights.pack(side=tk.LEFT, padx=10)  # Button to select YOLO weights file\n",
        "\n",
        "        btn_select_cfg = tk.Button(btn_frame, text=\"Select CFG\", command=self.select_cfg)\n",
        "        btn_select_cfg.pack(side=tk.LEFT, padx=10)  # Button to select YOLO cfg file\n",
        "\n",
        "        btn_select_names = tk.Button(btn_frame, text=\"Select Names\", command=self.select_names)\n",
        "        btn_select_names.pack(side=tk.LEFT, padx=10)  # Button to select YOLO names file\n",
        "\n",
        "        btn_select_image = tk.Button(btn_frame, text=\"Select Image\", command=self.select_image)\n",
        "        btn_select_image.pack(side=tk.LEFT, padx=10)  # Button to select an image file\n",
        "\n",
        "        btn_select_video = tk.Button(btn_frame, text=\"Select Video\", command=self.select_video)\n",
        "        btn_select_video.pack(side=tk.LEFT, padx=10)  # Button to select a video file\n",
        "\n",
        "        btn_live_video = tk.Button(btn_frame, text=\"Live Video\", command=self.toggle_live_video)\n",
        "        btn_live_video.pack(side=tk.LEFT, padx=10)  # Button to toggle live video from webcam\n",
        "\n",
        "        btn_detect_objects = tk.Button(btn_frame, text=\"Detect Objects\", command=self.toggle_detect_objects)\n",
        "        btn_detect_objects.pack(side=tk.LEFT, padx=10)  # Button to toggle object detection\n",
        "\n",
        "        btn_edge_detection = tk.Button(btn_frame, text=\"Edge Detection\", command=self.toggle_edge_detection)\n",
        "        btn_edge_detection.pack(side=tk.LEFT, padx=10)  # Button to toggle edge detection filter\n",
        "\n",
        "        btn_sharpen = tk.Button(btn_frame, text=\"Sharpen\", command=self.toggle_sharpen)\n",
        "        btn_sharpen.pack(side=tk.LEFT, padx=10)  # Button to toggle sharpen filter\n",
        "\n",
        "        # Other variables\n",
        "        self.image_path = None  # Path to the selected image\n",
        "        self.video_path = None  # Path to the selected video\n",
        "        self.image = None  # Loaded image\n",
        "        self.video_capture = None  # Video capture object\n",
        "        self.net = None  # YOLO network\n",
        "        self.classes = None  # YOLO class labels\n",
        "        self.output_layers = None  # YOLO output layers\n",
        "        self.filter_mode = None  # Current filter mode\n",
        "        self.detect_objects_flag = False  # To track object detection state\n",
        "        self.running = False  # To track live video state\n",
        "        self.thread = None  # Thread for parallel processing\n",
        "\n",
        "    # File selection methods\n",
        "    def select_weights(self):\n",
        "        \"\"\"\n",
        "        Select YOLO weights file\n",
        "        \"\"\"\n",
        "        self.weights_path = filedialog.askopenfilename()  # Open file dialog to select weights file\n",
        "        self.load_yolo()  # Load YOLO model if all files are provided\n",
        "\n",
        "    def select_cfg(self):\n",
        "        \"\"\"\n",
        "        Select YOLO cfg file\n",
        "        \"\"\"\n",
        "        self.cfg_path = filedialog.askopenfilename()  # Open file dialog to select cfg file\n",
        "        self.load_yolo()  # Load YOLO model if all files are provided\n",
        "\n",
        "    def select_names(self):\n",
        "        \"\"\"\n",
        "        Select YOLO names file\n",
        "        \"\"\"\n",
        "        self.names_path = filedialog.askopenfilename()  # Open file dialog to select names file\n",
        "        self.load_yolo()  # Load YOLO model if all files are provided\n",
        "\n",
        "    def load_yolo(self):\n",
        "        \"\"\"\n",
        "        Load YOLO model if all required files are provided\n",
        "        \"\"\"\n",
        "        if self.weights_path and self.cfg_path and self.names_path:  # Check if all files are provided\n",
        "            try:\n",
        "                self.net = cv2.dnn.readNet(self.weights_path, self.cfg_path)  # Load YOLO network\n",
        "                self.layer_names = self.net.getLayerNames()  # Get YOLO layer names\n",
        "                self.output_layers = [self.layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]  # Get YOLO output layers\n",
        "                with open(self.names_path, \"r\") as f:\n",
        "                    self.classes = [line.strip() for line in f.readlines()]  # Read YOLO class labels\n",
        "                messagebox.showinfo(\"YOLO\", \"YOLO model loaded successfully.\")  # Show success message\n",
        "            except Exception as e:\n",
        "                messagebox.showerror(\"YOLO Error\", f\"Error loading YOLO: {e}\")  # Show error message\n",
        "\n",
        "    # Image and video selection methods\n",
        "    def select_image(self):\n",
        "        \"\"\"\n",
        "        Select an image file\n",
        "        \"\"\"\n",
        "        self.image_path = filedialog.askopenfilename()  # Open file dialog to select image file\n",
        "        if len(self.image_path) > 0:  # Check if an image file is selected\n",
        "            self.load_image()  # Load the selected image\n",
        "\n",
        "    def select_video(self):\n",
        "        \"\"\"\n",
        "        Select a video file\n",
        "        \"\"\"\n",
        "        self.video_path = filedialog.askopenfilename()  # Open file dialog to select video file\n",
        "        if len(self.video_path) > 0:  # Check if a video file is selected\n",
        "            if self.running:  # Check if video is already running\n",
        "                self.stop_running()  # Stop running video\n",
        "            else:\n",
        "                self.running = True  # Set running state to True\n",
        "                self.thread = threading.Thread(target=self.detect_objects_video)  # Create a new thread for video processing\n",
        "                self.thread.start()  # Start the thread\n",
        "\n",
        "    def toggle_live_video(self):\n",
        "        \"\"\"\n",
        "        Start or stop the live video feed\n",
        "        \"\"\"\n",
        "        if self.running:  # Check if live video is already running\n",
        "            self.stop_running()  # Stop running live video\n",
        "        else:\n",
        "            self.video_capture = cv2.VideoCapture(0)  # Open webcam for live video\n",
        "            self.running = True  # Set running state to True\n",
        "            self.thread = threading.Thread(target=self.show_live_video)  # Create a new thread for live video\n",
        "            self.thread.start()  # Start the thread\n",
        "\n",
        "    def load_image(self):\n",
        "        \"\"\"\n",
        "        Load the selected image and display it\n",
        "        \"\"\"\n",
        "        image = cv2.imread(self.image_path)  # Read the selected image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB\n",
        "        image = Image.fromarray(image)  # Convert image to PIL format\n",
        "        image = ImageTk.PhotoImage(image)  # Convert image to ImageTk format\n",
        "\n",
        "        self.panel.config(image=image)  # Update the panel with the image\n",
        "        self.panel.image = image  # Keep a reference to avoid garbage collection\n",
        "\n",
        "    # Toggle methods for object detection and filters\n",
        "    def toggle_detect_objects(self):\n",
        "        \"\"\"\n",
        "        Toggle object detection on or off\n",
        "        \"\"\"\n",
        "        self.detect_objects_flag = not self.detect_objects_flag  # Toggle the detect_objects_flag\n",
        "\n",
        "    def toggle_edge_detection(self):\n",
        "        \"\"\"\n",
        "        Toggle edge detection filter on or off\n",
        "        \"\"\"\n",
        "        if self.filter_mode == \"edge\":  # Check if edge detection is already active\n",
        "            self.filter_mode = None  # Deactivate edge detection\n",
        "        else:\n",
        "            self.filter_mode = \"edge\"  # Activate edge detection\n",
        "            if not self.running:  # Check if live video is not running\n",
        "                self.toggle_live_video()  # Start live video\n",
        "\n",
        "    def toggle_sharpen(self):\n",
        "        \"\"\"\n",
        "        Toggle sharpening filter on or off\n",
        "        \"\"\"\n",
        "        if self.filter_mode == \"sharpen\":  # Check if sharpening is already active\n",
        "            self.filter_mode = None  # Deactivate sharpening\n",
        "        else:\n",
        "            self.filter_mode = \"sharpen\"  # Activate sharpening\n",
        "            if not self.running:  # Check if live video is not running\n",
        "                self.toggle_live_video()  # Start live video\n",
        "\n",
        "    # Object detection methods\n",
        "    def _detect_objects(self, frame):\n",
        "        \"\"\"\n",
        "        Apply YOLO object detection to the frame\n",
        "        \"\"\"\n",
        "        return self.apply_yolo(frame)  # Apply YOLO object detection\n",
        "\n",
        "    def detect_objects_video(self):\n",
        "        \"\"\"\n",
        "        Detect objects in the selected video file\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(self.video_path)  # Open the selected video file\n",
        "        while self.running:  # Check if the video is running\n",
        "            ret, frame = cap.read()  # Read a frame from the video\n",
        "            if not ret:  # Check if frame is not read successfully\n",
        "                break  # Exit the loop\n",
        "\n",
        "            if self.detect_objects_flag:  # Check if object detection is active\n",
        "                frame = self.apply_yolo(frame)  # Apply YOLO object detection\n",
        "\n",
        "            cv2.imshow(\"Video\", frame)  # Display the frame\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):  # Check if 'q' key is pressed\n",
        "                break  # Exit the loop\n",
        "\n",
        "        cap.release()  # Release the video capture object\n",
        "        cv2.destroyAllWindows()  # Close all OpenCV windows\n",
        "        self.running = False  # Set running state to False\n",
        "\n",
        "    def show_live_video(self):\n",
        "        \"\"\"\n",
        "        Display live video feed from the webcam\n",
        "        \"\"\"\n",
        "        while self.running:  # Check if live video is running\n",
        "            ret, frame = self.video_capture.read()  # Read a frame from the webcam\n",
        "            if not ret:  # Check if frame is not read successfully\n",
        "                break  # Exit the loop\n",
        "\n",
        "            if self.detect_objects_flag:  # Check if object detection is active\n",
        "                frame = self.apply_yolo(frame)  # Apply YOLO object detection\n",
        "\n",
        "            if self.filter_mode == \"edge\":  # Check if edge detection is active\n",
        "                frame = cv2.Canny(frame, 100, 200)  # Apply edge detection\n",
        "            elif self.filter_mode == \"sharpen\":  # Check if sharpening is active\n",
        "                kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])  # Sharpening kernel\n",
        "                frame = cv2.filter2D(frame, -1, kernel)  # Apply sharpening\n",
        "\n",
        "            cv2.imshow(\"Live Video\", frame)  # Display the frame\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):  # Check if 'q' key is pressed\n",
        "                break  # Exit the loop\n",
        "\n",
        "        self.stop_running()  # Stop running live video\n",
        "\n",
        "    def stop_running(self):\n",
        "        \"\"\"\n",
        "        Stop all running processes, release the video capture, and close all windows\n",
        "        \"\"\"\n",
        "        self.running = False  # Set running state to False\n",
        "        self.detect_objects_flag = False  # Deactivate object detection\n",
        "        self.filter_mode = None  # Deactivate filter mode\n",
        "        if self.video_capture is not None:  # Check if video capture object is not None\n",
        "            self.video_capture.release()  # Release the video capture object\n",
        "            self.video_capture = None  # Set video capture object to None\n",
        "        cv2.destroyAllWindows()  # Close all OpenCV windows\n",
        "\n",
        "    def apply_yolo(self, image):\n",
        "        \"\"\"\n",
        "        Apply YOLO object detection to the provided image\n",
        "        \"\"\"\n",
        "        if not self.net:  # Check if YOLO model is not loaded\n",
        "            messagebox.showerror(\"YOLO Error\", \"YOLO model is not loaded. Please load the model first.\")  # Show error message\n",
        "            return image  # Return the original image\n",
        "\n",
        "        height, width, channels = image.shape  # Get the dimensions of the image\n",
        "\n",
        "        # Create a 4D blob from a frame\n",
        "        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)  # Create a blob from the image\n",
        "        self.net.setInput(blob)  # Set the blob as input to the network\n",
        "        outs = self.net.forward(self.output_layers)  # Get the output from the network\n",
        "\n",
        "        class_ids = []  # List to hold class IDs\n",
        "        confidences = []  # List to hold confidences\n",
        "        boxes = []  # List to hold bounding boxes\n",
        "\n",
        "        # Iterate over each detection\n",
        "        for out in outs:\n",
        "            for detection in out:\n",
        "                scores = detection[5:]  # Get the scores for all classes\n",
        "                class_id = np.argmax(scores)  # Get the class ID with the highest score\n",
        "                confidence = scores[class_id]  # Get the confidence of the highest score\n",
        "                if confidence > 0.5:  # Check if confidence is above the threshold\n",
        "                    center_x = int(detection[0] * width)  # Calculate center x-coordinate\n",
        "                    center_y = int(detection[1] * height)  # Calculate center y-coordinate\n",
        "                    w = int(detection[2] * width)  # Calculate width of the bounding box\n",
        "                    h = int(detection[3] * height)  # Calculate height of the bounding box\n",
        "\n",
        "                    x = int(center_x - w / 2)  # Calculate top-left x-coordinate\n",
        "                    y = int(center_y - h / 2)  # Calculate top-left y-coordinate\n",
        "\n",
        "                    if (x, y, w, h) and isinstance(x, int) and isinstance(y, int) and isinstance(w, int) and isinstance(h, int):  # Check if coordinates and dimensions are valid\n",
        "                        boxes.append([x, y, w, h])  # Add bounding box to the list\n",
        "                        confidences.append(float(confidence))  # Add confidence to the list\n",
        "                        class_ids.append(class_id)  # Add class ID to the list\n",
        "\n",
        "        # Apply non-maxima suppression to suppress weak overlapping boxes\n",
        "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)  # Perform non-maxima suppression\n",
        "\n",
        "        # Draw bounding boxes and labels on the image\n",
        "        for i in range(len(boxes)):  # Iterate over bounding boxes\n",
        "            if i in indexes:  # Check if index is in the list of remaining indexes\n",
        "                x, y, w, h = boxes[i]  # Get the coordinates and dimensions of the bounding box\n",
        "                label = str(self.classes[class_ids[i]])  # Get the class label\n",
        "                confidence = confidences[i]  # Get the confidence\n",
        "                color = (0, 255, 0)  # Set the color for the bounding box\n",
        "                print(f\"Drawing rectangle at {(x, y)} to {(x + w, y + h)}\")  # Print coordinates for debugging\n",
        "                if isinstance(x, int) and isinstance(y, int) and isinstance(w, int) and isinstance(h, int):  # Check if coordinates and dimensions are valid\n",
        "                    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)  # Draw the bounding box\n",
        "                    cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)  # Draw the label and confidence\n",
        "\n",
        "        return image  # Return the image with bounding boxes\n",
        "\n",
        "    def apply_filter_to_image(self, filter_type):\n",
        "        \"\"\"\n",
        "        Apply the selected filter to the loaded image\n",
        "        \"\"\"\n",
        "        image = cv2.imread(self.image_path)  # Read the selected image\n",
        "        if filter_type == \"edge\":  # Check if edge detection filter is selected\n",
        "            image = cv2.Canny(image, 100, 200)  # Apply edge detection filter\n",
        "        elif filter_type == \"sharpen\":  # Check if sharpening filter is selected\n",
        "            kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])  # Sharpening kernel\n",
        "            image = cv2.filter2D(image, -1, kernel)  # Apply sharpening filter\n",
        "\n",
        "        image = Image.fromarray(image)  # Convert image to PIL format\n",
        "        image = ImageTk.PhotoImage(image)  # Convert image to ImageTk format\n",
        "\n",
        "        self.panel.config(image=image)  # Update the panel with the image\n",
        "        self.panel.image = image  # Keep a reference to avoid garbage collection\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    root = tk.Tk()  # Create the Tkinter root window\n",
        "    app = YOLOFaceDetectionApp(root)  # Create the YOLOFaceDetectionApp\n",
        "    root.mainloop()  # Start the Tkinter main loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r92CBZo1XzR"
      },
      "source": [
        "# Some Questions:\n",
        "**Why Convert the Image to RGB?**\n",
        "\n",
        "OpenCV loads images in BGR (Blue, Green, Red) format by default. However, many deep learning models, including those trained with popular frameworks like TensorFlow, PyTorch, and Caffe, expect input images to be in RGB (Red, Green, Blue) format. Converting the image from BGR to RGB ensures compatibility with these models.\n",
        "\n",
        "**Why Perform Other Preprocessing Steps?**\n",
        "\n",
        "* Normalization (Scaling Factor):\n",
        "\n",
        "Reason: Models perform better with normalized inputs. Normalization often involves scaling pixel values to a range of [0, 1] or [-1, 1].\n",
        "Implementation: scalefactor=0.00392 scales pixel values.\n",
        "​\n",
        " , converting them from [0, 255] to approximately [0, 1].\n",
        "* Resizing (Size):\n",
        "\n",
        "Reason: Neural networks expect input images to be of a fixed size. Resizing ensures that the image dimensions match the expected input size of the model.\n",
        "Implementation: size=(416, 416) resizes the image to 416x416 pixels.\n",
        "* Mean Subtraction (Mean):\n",
        "\n",
        "Reason: Mean subtraction is a normalization technique that improves model performance by centering the data around zero.\n",
        "Implementation: mean=(0, 0, 0) in this case means no mean subtraction is applied, but it could be used to subtract the average pixel values for each channel.\n",
        "* Channel Swapping (swapRB):\n",
        "\n",
        "Reason: Converts the image from BGR to RGB format.\n",
        "Implementation: swapRB=True swaps the red and blue channels.\n",
        "* Cropping (Crop):\n",
        "\n",
        "Reason: Ensures the image maintains the aspect ratio or is adjusted appropriately for the model input size.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
